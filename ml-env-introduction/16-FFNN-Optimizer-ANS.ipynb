{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow提供了多种优化算法:\n",
    "- tf.train.GradientDescentOptimizer\n",
    "- tf.train.AdadeltaOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.AdagradDAOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- tf.train.AdamOptimizer\n",
    "- tf.train.FtrlOptimizer\n",
    "- tf.train.ProximalGradientDescentOptimizer\n",
    "- tf.train.ProximalAdagradOptimizer\n",
    "- tf.train.RMSPropOptimizer\n",
    "\n",
    "参考：https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "\n",
    "其中AdamOptimizer用的较多，其中使用方法如下：\n",
    "```\n",
    "# cross_entropy是损失函数\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "```\n",
    "详细API描述，请参考：https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 请将下列代码的优化器更换成`AdamOptimizer`，并通过TensorBoard查看准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5c37adfb2a0b>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.098\n",
      "0.098\n",
      "0.098\n",
      "0.098\n",
      "0.098\n",
      "0.0981\n",
      "0.1002\n",
      "0.1064\n",
      "0.1439\n",
      "0.2036\n",
      "0.4081\n",
      "0.7103\n",
      "0.7328\n",
      "0.7437\n",
      "0.7491\n",
      "0.7622\n",
      "0.7736\n",
      "0.7806\n",
      "0.7862\n",
      "0.7903\n",
      "0.7999\n",
      "0.8049\n",
      "0.8079\n",
      "0.8116\n",
      "0.8161\n",
      "0.8233\n",
      "0.8249\n",
      "0.8289\n",
      "0.8325\n",
      "0.8363\n",
      "0.8383\n",
      "0.8404\n",
      "0.8476\n",
      "0.8472\n",
      "0.8519\n",
      "0.8544\n",
      "0.8547\n",
      "0.8595\n",
      "0.8591\n",
      "0.8616\n",
      "0.8642\n",
      "0.8654\n",
      "0.8661\n",
      "0.8693\n",
      "0.8718\n",
      "0.8739\n",
      "0.8736\n",
      "0.873\n",
      "0.8777\n",
      "0.8792\n",
      "0.8799\n",
      "0.8819\n",
      "0.8821\n",
      "0.8843\n",
      "0.8858\n",
      "0.8864\n",
      "0.8861\n",
      "0.8878\n",
      "0.8889\n",
      "0.889\n",
      "0.8909\n",
      "0.8921\n",
      "0.8931\n",
      "0.8929\n",
      "0.8939\n",
      "0.8958\n",
      "0.8946\n",
      "0.8962\n",
      "0.8964\n",
      "0.8972\n",
      "0.899\n",
      "0.8994\n",
      "0.899\n",
      "0.9005\n",
      "0.9017\n",
      "0.9029\n",
      "0.9037\n",
      "0.9037\n",
      "0.9029\n",
      "0.9039\n",
      "0.9047\n",
      "0.9045\n",
      "0.9056\n",
      "0.9066\n",
      "0.9072\n",
      "0.9079\n",
      "0.9084\n",
      "0.9087\n",
      "0.9084\n",
      "0.9093\n",
      "0.9096\n",
      "0.9109\n",
      "0.9102\n",
      "0.9117\n",
      "0.9109\n",
      "0.9121\n",
      "0.912\n",
      "0.9119\n",
      "0.9134\n",
      "0.9142\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    with tf.name_scope('layer'):\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = tf.Variable(tf.truncated_normal([in_size, out_size]), name='W')\n",
    "            \n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[1, out_size]), name='b')\n",
    "            \n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "            \n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b, )\n",
    "    \n",
    "    tf.summary.histogram('weights', Weights)\n",
    "    tf.summary.histogram('biases', biases)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('input'):\n",
    "        # X: 输入\n",
    "        X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "        # Y_: 标签\n",
    "        Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # L1：200个神经元\n",
    "    Y1 = add_layer(X, 784, 200, tf.nn.sigmoid)\n",
    "\n",
    "    # L2：100个神经元\n",
    "    Y2 = add_layer(Y1, 200, 100, tf.nn.sigmoid)\n",
    "\n",
    "    # L3: 60个神经元\n",
    "    Y3 = add_layer(Y2, 100, 60, tf.nn.sigmoid)\n",
    "\n",
    "    # L4: 30个神经元\n",
    "    Y4 = add_layer(Y3, 60, 30, tf.nn.sigmoid)\n",
    "\n",
    "    # L5: 10个神经元\n",
    "    Ylogits = add_layer(Y4, 30, 10, tf.nn.sigmoid)\n",
    "\n",
    "    # Output\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "    # 损失函数\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_, logits=Ylogits))\n",
    "        \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        \n",
    "    # 优化算法\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "        \n",
    "    # 计算准确率\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 将当前会话中的计算图保存\n",
    "        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "        for i in range(10000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict={X: batch_xs, Y_: batch_ys})\n",
    "\n",
    "            writer.add_summary(summary, i)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(accuracy.eval({X: mnist.test.images, Y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter-学习速率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。例如在我们的MNIST任务中，AdamOptimizer的学习速率参数就是一个待确定的值。\n",
    "\n",
    "为了更好的比较不同超参数下模型的性能，一种比较简单的的方法就是得到各个参数下模型的性能，然后进行对比。TensorBoard支持同时显示不同超参数下的数据，只需要将不同参数下产生的TensorBoard数据写入到不同的子目录即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个很简单的例子，实现了$y=x+step$的功能，将不同的step的数据写入到`logs`的不同子目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "for step in [k for k in range(10)]:\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, shape=(), name='x')\n",
    "        y = x+step\n",
    "        tf.summary.scalar('y', y)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            writer = tf.summary.FileWriter('logs/step_{0}/'.format(step))\n",
    "\n",
    "            for i in range(100):\n",
    "                summary, _ = sess.run([merged, y], feed_dict={x: i/100})\n",
    "\n",
    "                writer.add_summary(summary, i)\n",
    "\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 根据上述代码，我们探索一下在不同学习速率下模型的性能变化。输出的日志文件保存在`logs/adm_lr_{value}/`子目录下，其中`value`是具体的学习速率值。例如lr=0.001，则对应的TensorFlow事件输出到`logs/lr_0.001`目录下 (提示，修改tf.summary.FileWriter的参数)。\n",
    "学习速率参数在`AdamOptimizer`中指定，其中学习速率取值从0.001开始，直到0.01，步长为0.001\n",
    "\n",
    "运行完成之后TensorBoard显示效果图如下：\n",
    "![image.png](http://p811pjpxl.bkt.clouddn.com/16-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9d36dde7b959>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.1414\n",
      "0.8695\n",
      "0.9122\n",
      "0.9173\n",
      "0.9263\n",
      "0.9369\n",
      "0.937\n",
      "0.94\n",
      "0.9432\n",
      "0.9348\n",
      "0.9453\n",
      "0.9277\n",
      "0.9431\n",
      "0.9399\n",
      "0.9454\n",
      "0.9412\n",
      "0.9507\n",
      "0.9482\n",
      "0.9462\n",
      "0.9449\n",
      "0.9453\n",
      "0.9518\n",
      "0.948\n",
      "0.9528\n",
      "0.9516\n",
      "0.952\n",
      "0.9489\n",
      "0.9521\n",
      "0.953\n",
      "0.9442\n",
      "0.9536\n",
      "0.9514\n",
      "0.9536\n",
      "0.9419\n",
      "0.9503\n",
      "0.9573\n",
      "0.9516\n",
      "0.9537\n",
      "0.9545\n",
      "0.9544\n",
      "0.9572\n",
      "0.9447\n",
      "0.9563\n",
      "0.9516\n",
      "0.9488\n",
      "0.9569\n",
      "0.9493\n",
      "0.9528\n",
      "0.9499\n",
      "0.9536\n",
      "0.9496\n",
      "0.9571\n",
      "0.9525\n",
      "0.9606\n",
      "0.9556\n",
      "0.9575\n",
      "0.9567\n",
      "0.9581\n",
      "0.9576\n",
      "0.9533\n",
      "0.9524\n",
      "0.9554\n",
      "0.9548\n",
      "0.9552\n",
      "0.9547\n",
      "0.958\n",
      "0.9602\n",
      "0.9555\n",
      "0.9617\n",
      "0.9588\n",
      "0.941\n",
      "0.9492\n",
      "0.9602\n",
      "0.9582\n",
      "0.9587\n",
      "0.9578\n",
      "0.9554\n",
      "0.9577\n",
      "0.9549\n",
      "0.9596\n",
      "0.9587\n",
      "0.9563\n",
      "0.9625\n",
      "0.9578\n",
      "0.9582\n",
      "0.956\n",
      "0.9578\n",
      "0.9563\n",
      "0.9544\n",
      "0.9443\n",
      "0.9541\n",
      "0.9592\n",
      "0.9593\n",
      "0.9511\n",
      "0.9598\n",
      "0.9572\n",
      "0.9549\n",
      "0.9514\n",
      "0.96\n",
      "0.9633\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    with tf.name_scope('layer'):\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = tf.Variable(tf.truncated_normal([in_size, out_size]), name='W')\n",
    "            \n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[1, out_size]), name='b')\n",
    "            \n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "            \n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b, )\n",
    "    \n",
    "    tf.summary.histogram('weights', Weights)\n",
    "    tf.summary.histogram('biases', biases)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('input'):\n",
    "        # X: 输入\n",
    "        X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "        # Y_: 标签\n",
    "        Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # L1：200个神经元\n",
    "    Y1 = add_layer(X, 784, 200, tf.nn.sigmoid)\n",
    "\n",
    "    # L2：100个神经元\n",
    "    Y2 = add_layer(Y1, 200, 100, tf.nn.sigmoid)\n",
    "\n",
    "    # L3: 60个神经元\n",
    "    Y3 = add_layer(Y2, 100, 60, tf.nn.sigmoid)\n",
    "\n",
    "    # L4: 30个神经元\n",
    "    Y4 = add_layer(Y3, 60, 30, tf.nn.sigmoid)\n",
    "\n",
    "    # L5: 10个神经元\n",
    "    Ylogits = add_layer(Y4, 30, 10, tf.nn.sigmoid)\n",
    "\n",
    "    # Output\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "    # 损失函数\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_, logits=Ylogits))\n",
    "        \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        \n",
    "    # 优化算法\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "        \n",
    "    # 计算准确率\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 将当前会话中的计算图保存\n",
    "        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "        for i in range(10000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict={X: batch_xs, Y_: batch_ys})\n",
    "\n",
    "            writer.add_summary(summary, i)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(accuracy.eval({X: mnist.test.images, Y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-c12ad2e290a5>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/lyon/.pyenv/versions/3.6.4/envs/scikit-learn/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.0998\n",
      "0.472\n",
      "0.6761\n",
      "0.7672\n",
      "0.8201\n",
      "0.8426\n",
      "0.8665\n",
      "0.8788\n",
      "0.8868\n",
      "0.8965\n",
      "0.903\n",
      "0.9055\n",
      "0.9106\n",
      "0.9149\n",
      "0.9139\n",
      "0.9176\n",
      "0.9176\n",
      "0.9214\n",
      "0.9248\n",
      "0.9247\n",
      "0.9266\n",
      "0.9286\n",
      "0.9298\n",
      "0.9296\n",
      "0.9306\n",
      "0.9332\n",
      "0.9323\n",
      "0.9341\n",
      "0.9352\n",
      "0.9367\n",
      "0.9347\n",
      "0.9366\n",
      "0.9371\n",
      "0.9376\n",
      "0.939\n",
      "0.9406\n",
      "0.9411\n",
      "0.9409\n",
      "0.9427\n",
      "0.941\n",
      "0.9428\n",
      "0.9418\n",
      "0.9427\n",
      "0.9437\n",
      "0.9433\n",
      "0.9439\n",
      "0.9456\n",
      "0.9449\n",
      "0.9438\n",
      "0.9441\n",
      "0.9484\n",
      "0.9456\n",
      "0.9458\n",
      "0.9447\n",
      "0.946\n",
      "0.9472\n",
      "0.9471\n",
      "0.9474\n",
      "0.9485\n",
      "0.9481\n",
      "0.9473\n",
      "0.949\n",
      "0.9485\n",
      "0.9486\n",
      "0.9476\n",
      "0.9495\n",
      "0.9489\n",
      "0.9491\n",
      "0.9489\n",
      "0.9505\n",
      "0.9502\n",
      "0.9498\n",
      "0.9503\n",
      "0.951\n",
      "0.9509\n",
      "0.9506\n",
      "0.9524\n",
      "0.9528\n",
      "0.9504\n",
      "0.9503\n",
      "0.9516\n",
      "0.9513\n",
      "0.9525\n",
      "0.9535\n",
      "0.9528\n",
      "0.9545\n",
      "0.9531\n",
      "0.9524\n",
      "0.9515\n",
      "0.9536\n",
      "0.9537\n",
      "0.9543\n",
      "0.9549\n",
      "0.9529\n",
      "0.954\n",
      "0.9538\n",
      "0.9535\n",
      "0.9531\n",
      "0.9547\n",
      "0.9545\n",
      "0.0939\n",
      "0.6621\n",
      "0.7421\n",
      "0.8333\n",
      "0.8659\n",
      "0.8857\n",
      "0.8925\n",
      "0.9001\n",
      "0.9125\n",
      "0.9177\n",
      "0.9223\n",
      "0.9261\n",
      "0.9295\n",
      "0.9315\n",
      "0.9355\n",
      "0.9365\n",
      "0.9381\n",
      "0.9379\n",
      "0.9391\n",
      "0.9394\n",
      "0.9409\n",
      "0.9442\n",
      "0.9437\n",
      "0.9419\n",
      "0.9437\n",
      "0.9475\n",
      "0.9466\n",
      "0.9474\n",
      "0.9483\n",
      "0.9478\n",
      "0.9488\n",
      "0.9501\n",
      "0.9495\n",
      "0.9493\n",
      "0.9517\n",
      "0.9527\n",
      "0.9524\n",
      "0.9518\n",
      "0.9525\n",
      "0.9515\n",
      "0.9516\n",
      "0.9532\n",
      "0.9531\n",
      "0.955\n",
      "0.9539\n",
      "0.9516\n",
      "0.9552\n",
      "0.9543\n",
      "0.9534\n",
      "0.9559\n",
      "0.9528\n",
      "0.9542\n",
      "0.9546\n",
      "0.9554\n",
      "0.9529\n",
      "0.9562\n",
      "0.9544\n",
      "0.9555\n",
      "0.957\n",
      "0.9563\n",
      "0.9565\n",
      "0.9575\n",
      "0.956\n",
      "0.9553\n",
      "0.957\n",
      "0.9549\n",
      "0.9548\n",
      "0.9578\n",
      "0.9523\n",
      "0.9588\n",
      "0.9559\n",
      "0.9597\n",
      "0.9585\n",
      "0.9588\n",
      "0.9577\n",
      "0.9579\n",
      "0.9574\n",
      "0.9566\n",
      "0.9587\n",
      "0.954\n",
      "0.9587\n",
      "0.9587\n",
      "0.9598\n",
      "0.9589\n",
      "0.9579\n",
      "0.958\n",
      "0.9603\n",
      "0.9573\n",
      "0.9593\n",
      "0.9589\n",
      "0.9586\n",
      "0.9607\n",
      "0.9611\n",
      "0.9593\n",
      "0.9594\n",
      "0.9614\n",
      "0.9601\n",
      "0.9599\n",
      "0.9578\n",
      "0.962\n",
      "0.1006\n",
      "0.6961\n",
      "0.8412\n",
      "0.8782\n",
      "0.8982\n",
      "0.9086\n",
      "0.9189\n",
      "0.9237\n",
      "0.9261\n",
      "0.9278\n",
      "0.9327\n",
      "0.9366\n",
      "0.9349\n",
      "0.936\n",
      "0.9402\n",
      "0.9403\n",
      "0.943\n",
      "0.9467\n",
      "0.9467\n",
      "0.9478\n",
      "0.9495\n",
      "0.9503\n",
      "0.9473\n",
      "0.9494\n",
      "0.9513\n",
      "0.9507\n",
      "0.9528\n",
      "0.9535\n",
      "0.9513\n",
      "0.952\n",
      "0.9518\n",
      "0.9534\n",
      "0.9553\n",
      "0.9561\n",
      "0.9557\n",
      "0.9573\n",
      "0.9541\n",
      "0.9596\n",
      "0.9569\n",
      "0.9588\n",
      "0.9572\n",
      "0.9547\n",
      "0.9532\n",
      "0.9551\n",
      "0.9592\n",
      "0.9596\n",
      "0.9596\n",
      "0.9574\n",
      "0.9612\n",
      "0.9601\n",
      "0.9593\n",
      "0.9583\n",
      "0.9588\n",
      "0.9577\n",
      "0.9571\n",
      "0.96\n",
      "0.9591\n",
      "0.9607\n",
      "0.96\n",
      "0.9574\n",
      "0.9596\n",
      "0.9629\n",
      "0.9584\n",
      "0.9601\n",
      "0.9623\n",
      "0.9618\n",
      "0.9616\n",
      "0.9597\n",
      "0.9604\n",
      "0.9626\n",
      "0.9611\n",
      "0.9617\n",
      "0.9583\n",
      "0.9606\n",
      "0.9571\n",
      "0.9612\n",
      "0.9614\n",
      "0.9603\n",
      "0.9621\n",
      "0.9623\n",
      "0.9606\n",
      "0.9618\n",
      "0.9636\n",
      "0.9633\n",
      "0.9619\n",
      "0.9642\n",
      "0.9573\n",
      "0.9622\n",
      "0.9603\n",
      "0.9621\n",
      "0.9635\n",
      "0.9587\n",
      "0.9646\n",
      "0.9611\n",
      "0.9632\n",
      "0.9623\n",
      "0.9667\n",
      "0.963\n",
      "0.9645\n",
      "0.965\n",
      "0.113\n",
      "0.8052\n",
      "0.8737\n",
      "0.9055\n",
      "0.9171\n",
      "0.9237\n",
      "0.9304\n",
      "0.9328\n",
      "0.9351\n",
      "0.9404\n",
      "0.9412\n",
      "0.9437\n",
      "0.9468\n",
      "0.9429\n",
      "0.9475\n",
      "0.9476\n",
      "0.9521\n",
      "0.9481\n",
      "0.9537\n",
      "0.9532\n",
      "0.9553\n",
      "0.9543\n",
      "0.9551\n",
      "0.956\n",
      "0.9559\n",
      "0.9581\n",
      "0.9529\n",
      "0.9516\n",
      "0.958\n",
      "0.9594\n",
      "0.959\n",
      "0.9553\n",
      "0.9587\n",
      "0.9564\n",
      "0.9585\n",
      "0.9607\n",
      "0.9597\n",
      "0.9586\n",
      "0.9598\n",
      "0.9603\n",
      "0.962\n",
      "0.9613\n",
      "0.9622\n",
      "0.9627\n",
      "0.9608\n",
      "0.9612\n",
      "0.962\n",
      "0.9593\n",
      "0.9614\n",
      "0.9575\n",
      "0.962\n",
      "0.9616\n",
      "0.9586\n",
      "0.9622\n",
      "0.9635\n",
      "0.9623\n",
      "0.9643\n",
      "0.9605\n",
      "0.9648\n",
      "0.962\n",
      "0.9638\n",
      "0.9609\n",
      "0.9613\n",
      "0.9631\n",
      "0.9653\n",
      "0.9647\n",
      "0.9658\n",
      "0.9658\n",
      "0.9638\n",
      "0.962\n",
      "0.9653\n",
      "0.9662\n",
      "0.9634\n",
      "0.9648\n",
      "0.9672\n",
      "0.9644\n",
      "0.9667\n",
      "0.9685\n",
      "0.966\n",
      "0.9654\n",
      "0.9666\n",
      "0.9663\n",
      "0.9648\n",
      "0.9655\n",
      "0.9664\n",
      "0.9655\n",
      "0.9671\n",
      "0.9659\n",
      "0.9642\n",
      "0.9644\n",
      "0.9676\n",
      "0.968\n",
      "0.9663\n",
      "0.9658\n",
      "0.9662\n",
      "0.9685\n",
      "0.9629\n",
      "0.9673\n",
      "0.9675\n",
      "0.9658\n",
      "0.1042\n",
      "0.7832\n",
      "0.8331\n",
      "0.8378\n",
      "0.8831\n",
      "0.9049\n",
      "0.9198\n",
      "0.9258\n",
      "0.9329\n",
      "0.9341\n",
      "0.9363\n",
      "0.9407\n",
      "0.9409\n",
      "0.9441\n",
      "0.941\n",
      "0.9462\n",
      "0.946\n",
      "0.9435\n",
      "0.9506\n",
      "0.9518\n",
      "0.9514\n",
      "0.9444\n",
      "0.95\n",
      "0.9516\n",
      "0.9522\n",
      "0.9532\n",
      "0.9527\n",
      "0.9554\n",
      "0.9557\n",
      "0.9563\n",
      "0.9593\n",
      "0.9568\n",
      "0.9583\n",
      "0.9578\n",
      "0.9557\n",
      "0.9583\n",
      "0.9594\n",
      "0.9564\n",
      "0.9589\n",
      "0.9574\n",
      "0.9606\n",
      "0.9582\n",
      "0.9616\n",
      "0.9609\n",
      "0.9598\n",
      "0.9582\n",
      "0.964\n",
      "0.9637\n",
      "0.9582\n",
      "0.9619\n",
      "0.9651\n",
      "0.9627\n",
      "0.9626\n",
      "0.965\n",
      "0.9615\n",
      "0.9619\n",
      "0.9631\n",
      "0.964\n",
      "0.9592\n",
      "0.9623\n",
      "0.9612\n",
      "0.9612\n",
      "0.9567\n",
      "0.9634\n",
      "0.9618\n",
      "0.9591\n",
      "0.9603\n",
      "0.9588\n",
      "0.9626\n",
      "0.9609\n",
      "0.9595\n",
      "0.9638\n",
      "0.9566\n",
      "0.9616\n",
      "0.9641\n",
      "0.9634\n",
      "0.967\n",
      "0.9654\n",
      "0.9646\n",
      "0.9659\n",
      "0.9651\n",
      "0.9668\n",
      "0.9678\n",
      "0.9679\n",
      "0.9649\n",
      "0.9628\n",
      "0.9675\n",
      "0.9626\n",
      "0.9642\n",
      "0.9668\n",
      "0.9669\n",
      "0.9657\n",
      "0.9615\n",
      "0.9653\n",
      "0.9676\n",
      "0.961\n",
      "0.9662\n",
      "0.9643\n",
      "0.9647\n",
      "0.9639\n",
      "0.0905\n",
      "0.7417\n",
      "0.7984\n",
      "0.82\n",
      "0.8285\n",
      "0.8361\n",
      "0.8375\n",
      "0.8388\n",
      "0.8392\n",
      "0.8434\n",
      "0.8459\n",
      "0.8521\n",
      "0.8504\n",
      "0.8513\n",
      "0.8504\n",
      "0.8571\n",
      "0.8543\n",
      "0.851\n",
      "0.8531\n",
      "0.8552\n",
      "0.8549\n",
      "0.8505\n",
      "0.8595\n",
      "0.8596\n",
      "0.8595\n",
      "0.8567\n",
      "0.8611\n",
      "0.8516\n",
      "0.8614\n",
      "0.8603\n",
      "0.8596\n",
      "0.8651\n",
      "0.8605\n",
      "0.865\n",
      "0.8613\n",
      "0.8589\n",
      "0.8656\n",
      "0.8635\n",
      "0.8676\n",
      "0.862\n",
      "0.8654\n",
      "0.8647\n",
      "0.8653\n",
      "0.8652\n",
      "0.8599\n",
      "0.8627\n",
      "0.8665\n",
      "0.8593\n",
      "0.868\n",
      "0.8604\n",
      "0.8665\n",
      "0.8668\n",
      "0.8666\n",
      "0.8598\n",
      "0.8654\n",
      "0.8625\n",
      "0.8651\n",
      "0.8658\n",
      "0.858\n",
      "0.861\n",
      "0.8654\n",
      "0.8662\n",
      "0.8683\n",
      "0.8706\n",
      "0.8656\n",
      "0.8664\n",
      "0.8647\n",
      "0.8648\n",
      "0.8659\n",
      "0.8638\n",
      "0.8692\n",
      "0.872\n",
      "0.8685\n",
      "0.8659\n",
      "0.8653\n",
      "0.8679\n",
      "0.8663\n",
      "0.859\n",
      "0.8667\n",
      "0.8682\n",
      "0.8701\n",
      "0.8671\n",
      "0.8691\n",
      "0.869\n",
      "0.8682\n",
      "0.8702\n",
      "0.8707\n",
      "0.8678\n",
      "0.8678\n",
      "0.8678\n",
      "0.8674\n",
      "0.8685\n",
      "0.8698\n",
      "0.8662\n",
      "0.8656\n",
      "0.867\n",
      "0.87\n",
      "0.8702\n",
      "0.8695\n",
      "0.8712\n",
      "0.1131\n",
      "0.8339\n",
      "0.8728\n",
      "0.8654\n",
      "0.9112\n",
      "0.924\n",
      "0.93\n",
      "0.9294\n",
      "0.9344\n",
      "0.942\n",
      "0.9382\n",
      "0.943\n",
      "0.9416\n",
      "0.943\n",
      "0.9437\n",
      "0.9427\n",
      "0.9499\n",
      "0.9478\n",
      "0.9505\n",
      "0.9469\n",
      "0.9514\n",
      "0.9536\n",
      "0.9481\n",
      "0.9522\n",
      "0.9565\n",
      "0.9523\n",
      "0.9563\n",
      "0.9567\n",
      "0.9574\n",
      "0.9528\n",
      "0.9503\n",
      "0.9562\n",
      "0.9554\n",
      "0.9573\n",
      "0.9547\n",
      "0.9577\n",
      "0.9513\n",
      "0.9536\n",
      "0.9565\n",
      "0.9556\n",
      "0.9573\n",
      "0.9587\n",
      "0.954\n",
      "0.9538\n",
      "0.9561\n",
      "0.9501\n",
      "0.9517\n",
      "0.9582\n",
      "0.9578\n",
      "0.956\n",
      "0.9547\n",
      "0.9584\n",
      "0.9598\n",
      "0.9545\n",
      "0.9551\n",
      "0.9596\n",
      "0.9628\n",
      "0.958\n",
      "0.9589\n",
      "0.961\n",
      "0.9564\n",
      "0.9582\n",
      "0.9613\n",
      "0.9608\n",
      "0.9609\n",
      "0.9612\n",
      "0.9618\n",
      "0.9624\n",
      "0.9621\n",
      "0.9592\n",
      "0.9628\n",
      "0.9571\n",
      "0.9646\n",
      "0.9641\n",
      "0.9575\n",
      "0.9631\n",
      "0.9604\n",
      "0.9635\n",
      "0.967\n",
      "0.9661\n",
      "0.9645\n",
      "0.9648\n",
      "0.9614\n",
      "0.9624\n",
      "0.9649\n",
      "0.9673\n",
      "0.9628\n",
      "0.9638\n",
      "0.9657\n",
      "0.9636\n",
      "0.9633\n",
      "0.9656\n",
      "0.964\n",
      "0.9631\n",
      "0.9671\n",
      "0.9629\n",
      "0.9636\n",
      "0.9673\n",
      "0.9623\n",
      "0.9619\n",
      "0.1043\n",
      "0.842\n",
      "0.9025\n",
      "0.922\n",
      "0.9267\n",
      "0.9214\n",
      "0.9341\n",
      "0.9382\n",
      "0.9366\n",
      "0.9442\n",
      "0.9369\n",
      "0.9458\n",
      "0.9466\n",
      "0.9504\n",
      "0.9435\n",
      "0.9458\n",
      "0.949\n",
      "0.948\n",
      "0.9534\n",
      "0.9497\n",
      "0.9484\n",
      "0.9537\n",
      "0.9543\n",
      "0.9539\n",
      "0.9527\n",
      "0.9519\n",
      "0.9463\n",
      "0.9487\n",
      "0.9493\n",
      "0.9551\n",
      "0.9504\n",
      "0.954\n",
      "0.9539\n",
      "0.956\n",
      "0.9578\n",
      "0.9591\n",
      "0.9554\n",
      "0.9577\n",
      "0.9561\n",
      "0.9567\n",
      "0.9543\n",
      "0.9554\n",
      "0.9593\n",
      "0.9536\n",
      "0.9536\n",
      "0.9577\n",
      "0.958\n",
      "0.9612\n",
      "0.9624\n",
      "0.9614\n",
      "0.9604\n",
      "0.9547\n",
      "0.954\n",
      "0.9524\n",
      "0.9611\n",
      "0.9599\n",
      "0.9588\n",
      "0.9556\n",
      "0.9586\n",
      "0.9588\n",
      "0.9572\n",
      "0.9633\n",
      "0.9597\n",
      "0.964\n",
      "0.9599\n",
      "0.9642\n",
      "0.9621\n",
      "0.9549\n",
      "0.9586\n",
      "0.9552\n",
      "0.9609\n",
      "0.961\n",
      "0.9607\n",
      "0.9631\n",
      "0.9564\n",
      "0.9578\n",
      "0.9588\n",
      "0.9581\n",
      "0.9605\n",
      "0.9575\n",
      "0.9565\n",
      "0.9609\n",
      "0.9633\n",
      "0.9574\n",
      "0.9613\n",
      "0.9592\n",
      "0.962\n",
      "0.9629\n",
      "0.962\n",
      "0.9623\n",
      "0.9598\n",
      "0.9637\n",
      "0.9576\n",
      "0.9559\n",
      "0.9576\n",
      "0.949\n",
      "0.9625\n",
      "0.9626\n",
      "0.9658\n",
      "0.9663\n",
      "0.1515\n",
      "0.8197\n",
      "0.8421\n",
      "0.8505\n",
      "0.8995\n",
      "0.9278\n",
      "0.926\n",
      "0.9383\n",
      "0.931\n",
      "0.9356\n",
      "0.9271\n",
      "0.9369\n",
      "0.9402\n",
      "0.9438\n",
      "0.9471\n",
      "0.941\n",
      "0.9427\n",
      "0.951\n",
      "0.9463\n",
      "0.9481\n",
      "0.9492\n",
      "0.9431\n",
      "0.9498\n",
      "0.9532\n",
      "0.9524\n",
      "0.9489\n",
      "0.9507\n",
      "0.948\n",
      "0.9495\n",
      "0.958\n",
      "0.9475\n",
      "0.9546\n",
      "0.9596\n",
      "0.9524\n",
      "0.9534\n",
      "0.9523\n",
      "0.9496\n",
      "0.9543\n",
      "0.9547\n",
      "0.9582\n",
      "0.9539\n",
      "0.9566\n",
      "0.9519\n",
      "0.9501\n",
      "0.9531\n",
      "0.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9596\n",
      "0.9524\n",
      "0.9552\n",
      "0.9596\n",
      "0.9554\n",
      "0.9556\n",
      "0.9604\n",
      "0.9573\n",
      "0.9536\n",
      "0.9555\n",
      "0.9564\n",
      "0.9557\n",
      "0.959\n",
      "0.9572\n",
      "0.9575\n",
      "0.9554\n",
      "0.9568\n",
      "0.9556\n",
      "0.9531\n",
      "0.9588\n",
      "0.9565\n",
      "0.9603\n",
      "0.9592\n",
      "0.9581\n",
      "0.9631\n",
      "0.9615\n",
      "0.9603\n",
      "0.9613\n",
      "0.9606\n",
      "0.9558\n",
      "0.9583\n",
      "0.9634\n",
      "0.9584\n",
      "0.9587\n",
      "0.9591\n",
      "0.9585\n",
      "0.9547\n",
      "0.958\n",
      "0.9561\n",
      "0.9618\n",
      "0.9577\n",
      "0.9622\n",
      "0.9596\n",
      "0.9562\n",
      "0.9528\n",
      "0.9549\n",
      "0.9579\n",
      "0.9604\n",
      "0.9562\n",
      "0.9634\n",
      "0.9613\n",
      "0.9618\n",
      "0.9571\n",
      "0.9612\n",
      "0.1029\n",
      "0.7972\n",
      "0.9\n",
      "0.916\n",
      "0.9239\n",
      "0.9286\n",
      "0.9288\n",
      "0.9353\n",
      "0.9349\n",
      "0.9392\n",
      "0.9359\n",
      "0.9361\n",
      "0.9415\n",
      "0.9467\n",
      "0.9464\n",
      "0.942\n",
      "0.9508\n",
      "0.9434\n",
      "0.9433\n",
      "0.9515\n",
      "0.9425\n",
      "0.9509\n",
      "0.9521\n",
      "0.9512\n",
      "0.9521\n",
      "0.9541\n",
      "0.9542\n",
      "0.9544\n",
      "0.9506\n",
      "0.9555\n",
      "0.9517\n",
      "0.9546\n",
      "0.9408\n",
      "0.9485\n",
      "0.9477\n",
      "0.9469\n",
      "0.9485\n",
      "0.9551\n",
      "0.95\n",
      "0.9516\n",
      "0.9542\n",
      "0.9496\n",
      "0.9563\n",
      "0.9534\n",
      "0.9539\n",
      "0.9467\n",
      "0.9508\n",
      "0.9522\n",
      "0.9529\n",
      "0.9543\n",
      "0.9553\n",
      "0.956\n",
      "0.9559\n",
      "0.9547\n",
      "0.9542\n",
      "0.9482\n",
      "0.9596\n",
      "0.9552\n",
      "0.9553\n",
      "0.9479\n",
      "0.9537\n",
      "0.9572\n",
      "0.9547\n",
      "0.9526\n",
      "0.957\n",
      "0.9523\n",
      "0.955\n",
      "0.9559\n",
      "0.9497\n",
      "0.95\n",
      "0.9569\n",
      "0.9511\n",
      "0.953\n",
      "0.9564\n",
      "0.9536\n",
      "0.9511\n",
      "0.9557\n",
      "0.948\n",
      "0.9528\n",
      "0.9555\n",
      "0.9504\n",
      "0.9545\n",
      "0.9587\n",
      "0.9543\n",
      "0.9511\n",
      "0.9602\n",
      "0.9588\n",
      "0.9526\n",
      "0.9518\n",
      "0.958\n",
      "0.954\n",
      "0.9553\n",
      "0.9555\n",
      "0.9579\n",
      "0.9554\n",
      "0.9607\n",
      "0.9596\n",
      "0.9588\n",
      "0.9556\n",
      "0.9587\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    with tf.name_scope('layer'):\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = tf.Variable(tf.truncated_normal([in_size, out_size]), name='W')\n",
    "            \n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.1, shape=[1, out_size]), name='b')\n",
    "            \n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "            \n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b, )\n",
    "    \n",
    "    tf.summary.histogram('weights', Weights)\n",
    "    tf.summary.histogram('biases', biases)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "for lr in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]:\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.name_scope('input'):\n",
    "            # X: 输入\n",
    "            X = tf.placeholder(tf.float32, [None, 784], name=\"X\")\n",
    "            # Y_: 标签\n",
    "            Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # L1：200个神经元\n",
    "        Y1 = add_layer(X, 784, 200, tf.nn.sigmoid)\n",
    "\n",
    "        # L2：100个神经元\n",
    "        Y2 = add_layer(Y1, 200, 100, tf.nn.sigmoid)\n",
    "\n",
    "        # L3: 60个神经元\n",
    "        Y3 = add_layer(Y2, 100, 60, tf.nn.sigmoid)\n",
    "\n",
    "        # L4: 30个神经元\n",
    "        Y4 = add_layer(Y3, 60, 30, tf.nn.sigmoid)\n",
    "\n",
    "        # L5: 10个神经元\n",
    "        Ylogits = add_layer(Y4, 30, 10, tf.nn.sigmoid)\n",
    "\n",
    "        # Output\n",
    "        Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "        # 损失函数\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_, logits=Ylogits))\n",
    "\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "        # 优化算法\n",
    "        with tf.name_scope('train'):\n",
    "            train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "        # 计算准确率\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            # 将当前会话中的计算图保存\n",
    "            writer = tf.summary.FileWriter(\"logs/adm_lr_{0}/\".format(lr), sess.graph)\n",
    "\n",
    "            for i in range(10000):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict={X: batch_xs, Y_: batch_ys})\n",
    "\n",
    "                writer.add_summary(summary, i)\n",
    "\n",
    "                if i%100 == 0:\n",
    "                    print(accuracy.eval({X: mnist.test.images, Y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 对比不同算法的性能 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 对比`GradientDescentOptimizer`和`tf.train.AdamOptimizer`的性能区别。\n",
    "提示：使用GradientDescentOptimizer算法，学习速率从0.1开始一直到1.0，输出到子目录`logs/gd_lr_{value}`下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 本地安装TF环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先安装Python 3的环境，然后通过pip进行安装\n",
    "```\n",
    "pip install tensorflow\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "安装完成后，启动jupyter服务端：\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
